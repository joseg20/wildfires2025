# Evaluation Utilities for Images

This directory contains helper scripts for running predictions and measuring detection performance. They assume YOLO-based models and operate directly on label files generated by `yolo predict`.

## compute_perf.py

Provides routines to evaluate predictions against ground truth labels. Metrics include precision, recall and F1 score. Functions can sweep confidence thresholds, plot metrics and compare multiple prediction folders.

## compute_scores.py

Command-line interface for comparing several prediction runs. Given a ground truth folder and a directory of prediction subfolders, it prints a table of F1 scores and plots metrics for each run.

```
python compute_scores.py --gt_folder labels/val --pred_folder models/my_experiment/predictions
```

## compute_scores_test.py

Variant of the previous script that reads a CSV file with per-model confidence thresholds. It evaluates each prediction folder using the provided threshold and writes the results to `results.csv`.

## run_predicctions.py

Runs YOLO predictions for every `best.pt` file under a model directory. Results are stored in `models/<project>/test_preds`.

```
python run_predicctions.py --data_directory data/val/images --model_directory models --project my_exp
```

## run_predictions_test.py

Similar helper for test datasets. The list `datasets_test` defines the image folders used for prediction.

## utils.py

Utility functions used by the evaluation scripts, including

- `xywh2xyxy`: convert bounding boxes from center format to corner coordinates
- `box_iou`: compute IoU between bounding boxes

## Requirements

Install dependencies from `../requirements.txt`:

```
pip install -r ../requirements.txt
```
